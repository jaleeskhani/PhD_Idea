# PhD_Idea
Just Checking

Data privacy and confidentiality are always priorities of data providers. Advanced and smart attacks are increasing day by data and feature filtering based traditional intrusion detection networks are facing problems in the detection of new attacks. In this project, I have focused on the detection of such attacks as threats. I have considered the NSL-KDD dataset, preprocessed it, and applied different Machine Learning (ML) models including Artificial Neural Network (ANN), Decision Tree Classifier (DTC), and Random Forest Classifier (RFC) after doing feature engineering. The experimental results showed that DTC is a better option for the binary classification of the dataset, i.e., normal and threat.
Introduction and background
Adversarial attack is adding noise to a Machine Learning (ML) model to destroy the results of the model. The noise may be added to any level of model including input and outputs physical domains, pre-processing digital representation, or ML model itself. Targets of these attacks are all three types of systems i.e., supervised, unsupervised and reinforcement (supervised plus unsupervised) learning. According to research, supervised learning is the focus of these attacks [1, 2] but it can also affect reinforcement learning equally as supervised learning [3]. These attacks target two phases of system operation i.e., training or testing of model [4].
Training attacks consist of data access attacks and poisoning attacks. Some or all portions of input data are controlled in data access attacks. This controlled data is used to create substitute fake model which is used in testing phase to destroy the results. In poising, aka causative attacks, alteration of model or data is done indirectly or directly. Indirect poisoning attack alters the data before pre-processing, without any access to pre-processed data. A direct poisoning attack consists of Data Injection/Manipulation attacks in which adversarial data is added, injected, or manipulated to the training set without any addition or changes to output labels. This results in alteration of error algorithms (i.e., disturbance in decision boundary in case of unsupervised learning and maximizing gradient decent error in case of supervised learning), which causes decrease in performance in both cases. Logical corruption attacks are also poising attacks in which tampering of ML algorithms are done to destroy the model itself and learning process of the model.
Unlike training attacks, testing, aka exploratory attacks, try to either add crafted noise to testing data to evade outputs (evasion attacks) or capture information of training data or model (oracle attacks). Here, the adversary finds minimal crafted noise using optimization problem, which results in maximum change in loss functions and hence output is misclassified [5]. Instead of gradient-based algorithms attacks [6], there are gradient free attacks which require model confidence values for evasion [8]. Attacker has almost all knowledge of complete ML model. Despite no access to the original model, attackers use application-based programming interface to observe and copy model inputs and outputs and substitute the model with altered one, generated by application. This attack is subdivided into extraction attacks, inversion attacks, and membership inference attacks.
Researchers are trying to tackle all these attacks and keep the data and network as safe as possible. There are defense mechanisms for the above-mentioned attacks. Simple way for data access attack is to hide data from attackers and data encryption is the solution. Data sanitization is a solution to poisoning training data. Reject on Negative Impact [9], is approach where adversarial data is removed from training data, after successful validation of data that causes high classification error. Robust Statistics (RS) try to avoid tempering of ML algorithm, caused by poisoned data. In this technique, RS uses regularization and restriction techniques to minimize alteration of learning models.
